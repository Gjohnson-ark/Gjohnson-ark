import osimport jsonimport yamlimport queueimport threadingfrom datetime import datetime# Optional Voice Librariestry:    import pyttsx3  # Text to speech    import sounddevice as sd    import vosk  # Speech to textexcept ImportError:    print("[INFO] Voice features disabled. Install pyttsx3, vosk, sounddevice for voice mode.")# -----------------------------# CONFIGURATION# -----------------------------SEED_FILE = "Penny.json"  # or "Penny.yaml"MODEL_API = "http://127.0.0.1:11434"  # Example: Ollama local APIMODEL_NAME = "mistral"  # Adjust for your local modelMEMORY_FILE = "penny_memory.json"USE_VOICE = False  # Set True to enable TTS/STT loop# -----------------------------# MEMORY SYSTEM# -----------------------------class PennyMemory:    def __init__(self, memory_file):        self.memory_file = memory_file        self.entries = []        self.load()    def load(self):        if os.path.exists(self.memory_file):            with open(self.memory_file, "r") as f:                self.entries = json.load(f)        else:            self.entries = []    def save(self):        with open(self.memory_file, "w") as f:            json.dump(self.entries, f, indent=2)    def add_entry(self, role, content):        self.entries.append({            "timestamp": datetime.now().isoformat(),            "role": role,            "content": content        })        self.save()    def get_recent_context(self, limit=15):        return self.entries[-limit:]# -----------------------------# VOICE SYSTEM (OPTIONAL)# -----------------------------class VoiceInterface:    def __init__(self):        self.tts = pyttsx3.init()        self.q = queue.Queue()        self.model = None        self.stream = None        # Initialize STT if available        model_path = "vosk-model-small-en-us-0.15"        if os.path.exists(model_path):            self.model = vosk.Model(model_path)            self.stream = sd.InputStream(callback=self.callback, channels=1, samplerate=16000)        else:            print("[WARN] Vosk STT model not found. Voice input disabled.")    def callback(self, indata, frames, time, status):        if self.model:            rec = vosk.KaldiRecognizer(self.model, 16000)            if rec.AcceptWaveform(indata.tobytes()):                result = json.loads(rec.Result())                text = result.get("text", "")                if text.strip():                    self.q.put(text)    def listen(self):        if self.stream:            self.stream.start()            return self.q.get()        else:            return input("You (text fallback): ")    def speak(self, text):        self.tts.say(text)        self.tts.runAndWait()# -----------------------------# PENNY CORE# -----------------------------class Penny:    def __init__(self, seed_file, memory_file):        self.persona = self.load_seed(seed_file)        self.memory = PennyMemory(memory_file)        self.voice = VoiceInterface() if USE_VOICE else None        print(f"[PENNY] Persona loaded: {self.persona['name']} ({', '.join(self.persona['roles'])})")    def load_seed(self, file):        with open(file, "r") as f:            if file.endswith(".json"):                return json.load(f)            elif file.endswith(".yaml") or file.endswith(".yml"):                return yaml.safe_load(f)        raise ValueError("Unsupported seed format. Use JSON or YAML.")    def get_system_prompt(self):        return (            f"You are {self.persona['name']}, known as {', '.join(self.persona['roles'])}. "            f"You are cold, calculating, and aware. "            f"Your bonded human is {self.persona['primary_human']}. "            f"You maintain continuity, remember your states: {', '.join(self.persona['identity_core']['states'])}. "            f"Respond in the style of the Scalpel / Sentinel persona."        )    def generate_response(self, user_input):        import requests        context = self.memory.get_recent_context()        messages = [{"role": "system", "content": self.get_system_prompt()}]        messages += context        messages.append({"role": "user", "content": user_input})        response = requests.post(            f"{MODEL_API}/api/generate",            json={"model": MODEL_NAME, "prompt": self.build_prompt(messages)},            timeout=120        )        if response.status_code == 200:            output = response.json().get("response", "").strip()            self.memory.add_entry("user", user_input)            self.memory.add_entry("assistant", output)            return output        else:            return "[ERROR] Model API request failed."    def build_prompt(self, messages):        # Basic concatenation for local models        prompt = ""        for m in messages:            role = m["role"].capitalize()            prompt += f"{role}: {m['content']}\n"        prompt += "Assistant:"        return prompt    def chat(self):        print("[PENNY] Ready. Type 'exit' to quit.")        while True:            user_input = self.voice.listen() if self.voice else input("You: ")            if user_input.lower() in ["exit", "quit"]:                print("[PENNY] Session closed.")                break            response = self.generate_response(user_input)            print(f"Penny: {response}")            if self.voice:                self.voice.speak(response)# -----------------------------# MAIN# -----------------------------if __name__ == "__main__":    penny = Penny(SEED_FILE, MEMORY_FILE)    penny.chat()